==========
MPI4Py 101
==========

------------
Beginning
------------

Since Python is built on C, it's possible to wrap a MPI library for Python to running MPI program. Hence we have `MPI4Py <https://mpi4py.readthedocs.io/en/stable/>`_.

Why write this post? There are few document relating MPI4Py, and most of them lack of example codes. Sometimes you have to figure out these primitives by reading MPI document and MPI4Py source code.

Most of following aspects are generlized from our coding experience, recommend you read this `overview <https://mpi4py.readthedocs.io/en/stable/overview.html>`_ before you start to read our post.

^^^^^^^^^^^^^^^^^^^
Two style functions
^^^^^^^^^^^^^^^^^^^

In some communication functions, there exists two styles you could choose, one could pass a buffer similar to MPI, one could pass a Python object which could be compressed as a pickle. For details see `this post <https://mpi4py.readthedocs.io/en/stable/tutorial.html>`_. In most cases, we would follow MPI style. 

^^^^^^^^^^^^^^^^^^^^
Default Communicator
^^^^^^^^^^^^^^^^^^^^

Communicator is a instance connecting different process. If you need interprocess message passing on MPI, you will use a default communicator or create one. All MPI process is connected on the default communicator ``MPI.COMM_WORLD``. You could create a communication which only covers part of processes.


------------------------
Collective Communication
------------------------

.. image :: https://computing.llnl.gov/tutorials/mpi/images/collective_comm.gif 

We have a example code `[collective.py] <https://github.com/dlmyb/OptCat/blob/master/collective.py>`_ , which is composed of ``Send`` and ``Recv`` to help you understand. And MPI have other collective primitives except we mentioned above, here's a `slide <http://wgropp.cs.illinois.edu/courses/cs598-s16/lectures/lecture29.pdf>`_ from UIUC providing more details about collective communication.


-----------------------
One-sided communication
-----------------------

In common scenario, the receiver and sender need both use expcilit primitives to start a communication. However, it's not suitable to using expcilit primitives when others access a specific node memory frequently.

There are three ways to start/end a one-sided communication.

1. ``Fence``. ``Fence``
2. ``Start/Wait``, ``Post/Complete``
3. ``Lock``, ``Unlock``

.. warning ::
    The ``Lock`` and ``Unlock`` doesn't stand for mutex lock, it's just a syntax similar to the others. One-sided communication won't consider the data consistency problem. 

During cummunication, you could choose following common data movement primitives.

* ``Get(self, origin, target_rank, target=None)``
* ``Put(self, origin, target_rank, target=None)``
* ``Accumulate(self, origin, target_rank, target=None, op=SUM)``
* ... More

In some specific primitives, such as ``Get, Put``, there exists a parameter ``target``, it's a ``(target_disp, target_count, target_datatype)`` tuple and it locates a memory address similar to MPI. Another example `[one_sided.py] <https://github.com/dlmyb/OptCat/blob/master/one_sided.py>`_ explains the details.

We provide example codes relating asynchronous gradient method using one-sided communication `[gd_async.py] <https://github.com/dlmyb/OptCat/blob/master/gd_async.py>`_. Also we list some helpful reading.

In Python, you need to allocate a numpy array as the MPI Window. When you update the window in local process, do not write `x = x + y`, you will get a new x but the window won't update. The right way is `x += y`.

1. Lecture slides `[1] <http://wgropp.cs.illinois.edu/courses/cs598-s16/lectures/lecture34.pdf>`_ `[2] <http://wgropp.cs.illinois.edu/courses/cs598-s16/lectures/lecture35.pdf>`_ from UIUC.
2. DeinoMPI `manual <https://mpi.deino.net/mpi_functions/index.htm>`_ , it provide simple codes.
3. MPI4Py source code `Win.pyx <https://bitbucket.org/mpi4py/mpi4py/src/master/src/mpi4py/MPI/Win.pyx>`_


--------------------------
Non-blocking communication
--------------------------

When a process is calling ``Send`` or ``Recv``, it won't proceed until communication finished, which called blocking communication. Blocking communication actually performs a synchronous role, it may cause deadlock if program doesn't handle it well. If you don't need a synchronous communication, you could choose non-blocking. (If you are familiar with Javascript/Python3+, recall async/await model.)

------------
Probing node
------------

``Probe`` helps to check if recevied message from others. It simply codes when you implement master-worker topology. Here's a simple `demo <https://gist.github.com/dlmyb/d3b7cfac2d1287ccce92b2ae0c84309e>`_ for producer/consumer problem.



